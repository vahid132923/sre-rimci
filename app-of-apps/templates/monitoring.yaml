apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "-700"
  name: monitoring
  namespace: argocd
 
spec:
  destination:
    namespace: monitoring
    server: {{ .Values.spec.destination.server }}
  project: default 
  syncPolicy:
   automated: {}
   syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=orphan
    - ServerSideApply=true
  sources:
  - repoURL: 'https://victoriametrics.github.io/helm-charts/'
    chart: victoria-metrics-k8s-stack
    targetRevision: 0.66.1
    helm:
      values: |-
         victoria-metrics-operator:
            image:
              registry: {{ .Values.spec.registry }}
              repository: victoriametrics/operator       
            env:
              - name: VM_CONTAINERREGISTRY
                value: "{{ .Values.spec.registry }}"
              - name: VM_CONFIG_RELOADER_IMAGE
                value: victoriametrics/operator:config-reloader-v0.66.1 
            cleanupImage:
               repository: {{ .Values.spec.registry }}/rancher/kubectl
         vmsingle:
            enabled: false      
         vmcluster:
            enabled: true
            spec:
              retentionPeriod: "30d"
              vmstorage:
                 storage:
                    volumeClaimTemplate:
                       spec:
                         storageClassName: "k8s-lvm-sc"  
                         resources:
                            requests:
                               storage: 40Gi
                 resources:
                    limits:
                       cpu: 2000m
                       memory: 2Gi
                    requests: 
                       cpu: 500m
                       memory: 512Mi
              vmselect:
                 storage:
                    volumeClaimTemplate:
                       spec:
                         storageClassName: "k8s-lvm-sc"   
                         resources:
                            requests:
                               storage: 5Gi
                 resources:
                    limits:
                       cpu: 500m
                       memory: 512Mi
                    requests: 
                       cpu: 500m
                       memory: 512Mi   
              vminsert:
                 resources:
                    limits:
                       cpu: 500m
                       memory: 512Mi
                    requests: 
                       cpu: 500m
                       memory: 512Mi  
            ingress:
               select:
                  enabled: true
                  ingressClassName: private
                  annotations: {}                 
                  hosts:
                    - vmselect.rimci.dev
                  tls:
                    - secretName: vmselect-acme-issuer
                      hosts:
                        - vmselect.rimci.dev                       

               insert:
                  enabled: true
                  ingressClassName: private
                  annotations:                 
                    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
                    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
                    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
                    nginx.ingress.kubernetes.io/client-header-buffer-size: "50m"
                  hosts:
                    - vminsert.rimci.dev
                  tls:
                    - secretName: vminsert-acme-issuer
                      hosts:
                        - vminsert.rimci.dev                                
         alertmanager:
             spec:
                image: 
                  repository: {{ .Values.spec.registry }}/prometheus/alertmanager
             config:
                 templates:
                   - "/etc/vm/configs/**/*.tmpl"
                 route:
                   group_by: ["alertgroup", "job"]
                   group_wait: 30s
                   group_interval: 5m
                   repeat_interval: 1h
                   receiver: "blackhole"
                   routes:
                     - matchers:
                         - severity=~"critical"
                       group_by: ["alertgroup", "job"]
                       receiver: devops
             
                 inhibit_rules:
                   - target_matchers:
                       - severity=~"warning|info"
                     source_matchers:
                       - severity=critical
                     equal:
                       - cluster
                       - namespace
                       - alertname
                   - target_matchers:
                       - severity=info
                     source_matchers:
                       - severity=warning
                     equal:
                       - cluster
                       - namespace
                       - alertname
                   - target_matchers:
                       - severity=info
                     source_matchers:
                       - alertname=InfoInhibitor
                     equal:
                       - cluster
                       - namespace

                 #receivers:
                 #  - name: blackhole
                 #  - name: devops
                 #    webhook_configs:
                 #      - url: 'http://192.168.200.44/webhook/ad794e23-cb18-4dbf-9644-263624fd5b90/alert_devops'
                 #        send_resolved: true
                   
             
         vmagent:
             spec:
                externalLabels:
                  cluster: tools
                #inlineScrapeConfig: |
                #  - job_name: "cdn"
                #    static_configs:
                #    - targets: ["192.168.207.250:9100"]      
                #  - job_name: "cdn_vts"
                #    static_configs:
                #    - targets: ["192.168.207.250:8080/status"]
                #  - job_name: "dorsa-postgres"
                #    static_configs:
                #    - targets: ["192.168.200.100:9187"]
                #  - job_name: "kafka-exporter"
                #    static_configs:
                #    - targets: ["192.168.200.100:9308"]
         grafana:
           global:
             imageRegistry: {{ .Values.spec.registry }}
           sidecar:
              dashboards:
                multicluster: true     
           adminPassword: 55UItzBingDjcbEzr5Tz09ETcHMU6imT3h7Oyxm0 
           assertNoLeakedSecrets: false
           grafana.ini:
            auth.generic_oauth:
             enabled: true
             name: SSO 
             allow_sign_up: false
             client_id: grafana
             client_secret: j3QByo61qpIY8IagKYPFmxwpXJ6EEt3n
             scopes: openid email profile offline_access roles
             email_attribute_path: email
             login_attribute_path: username
             name_attribute_path: full_name
             auth_url: https://sso.rimci.dev/realms/rimci/protocol/openid-connect/auth
             token_url: https://sso.rimci.dev/realms/rimci/protocol/openid-connect/token
             api_url: https://sso.rimci.dev/realms/rimci/protocol/openid-connect/userinfo
             role_attribute_path: contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'           
           persistence:
             enabled: true
             storageClassName: k8s-lvm-sc
             size: 5Gi  
           initChownData:
             enabled: false
             image:
               repository: busybox
               tag: latest
           datasources:
           downloadDashboardsImage:
               registry: {{ .Values.spec.registry }}
           ingress:
             enabled: true
             ingressClassName: private
             hosts:
               - fanoos.rimci.dev
         prometheus-node-exporter:
             image:
                registry: {{ .Values.spec.registry }}
         kube-state-metrics:
             image:
                registry: {{ .Values.spec.registry }}
         prometheus-operator-crds:
            enabled: true      